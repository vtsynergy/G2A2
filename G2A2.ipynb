{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-c4fr1ksa because the default path (/home/jovyan/.cache/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "#Required library\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "from networkx.algorithms import bipartite\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from scipy.stats import cauchy, gamma\n",
    "from random import randint, uniform\n",
    "%matplotlib inline\n",
    "from sklearn import preprocessing\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.sparse import csr_matrix\n",
    "import math\n",
    "import copy\n",
    "import multiprocessing\n",
    "import time\n",
    "import random\n",
    "from sdv.tabular import CTGAN\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cpu count:128\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of cpu count:\"+str(multiprocessing.cpu_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################---G2A2 - Dynamic Graph Generation -----#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################---------Important functions-------------######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cauchy distribution\n",
    "def cauchyDistribution(y, l, s):\n",
    "    y1 = cauchy.pdf(y, loc = l, scale = s)\n",
    "    return y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gamma distribution\n",
    "def gammaDistribution(y, a, l, s):\n",
    "    y1 = gamma.pdf(y, a, loc = l, scale = s)\n",
    "    return y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples node and its probability for the graph snapshot\n",
    "def sampleNodes(x, size, p):  \n",
    "    \n",
    "    p[len(p) - 1] = 1 - np.sum(p[0:len(p)-1]) \n",
    "    \n",
    "    sampledNodes = np.random.choice(x, size, p = p, replace = False)\n",
    "    \n",
    "    for i in range(len(x)):\n",
    "        if x[i] not in sampledNodes:\n",
    "            p[i] = 0\n",
    "    \n",
    "    p = probToOne(p)\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample degree by sampling nodes with repetation. Then counts the nodes to create the node sequence.\n",
    "def sampleDegree(x, size, p):\n",
    "    \n",
    "    p = probToOne(p)\n",
    "    p[len(p) - 1] = 1 - np.sum(p[0:len(p)-1]) \n",
    "    \n",
    "    if p[len(p) - 1] < 0:\n",
    "        p[len(p) - 1] = 0\n",
    "        p = probToOne(p)\n",
    "\n",
    "    sampleEdges = np.random.choice(x, size, p = p)\n",
    "    \n",
    "    seq = np.zeros(len(x), dtype = np.int32)\n",
    "    \n",
    "    for i in range(len(sampleEdges)):\n",
    "        seq[sampleEdges[i]] += 1\n",
    "    \n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the array sum equal to one\n",
    "def probToOne(x):\n",
    "    for i in range(len(x)):\n",
    "        if x[i] < 0:\n",
    "            x[i] = 0\n",
    "            \n",
    "    x = x/x.sum()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates graph snapshot for the time stamp t\n",
    "\n",
    "def genSS(p_u, p_v, c_u, c_v, c_e, uNodeIndex, vNodeIndex, t):   \n",
    "\n",
    "    if c_u[t] != 0 and c_v[t] != 0 and c_e[t] != 0:\n",
    "        P_su = sampleNodes(uNodeIndex, c_u[t], copy.deepcopy(p_u))\n",
    "        P_sv = sampleNodes(vNodeIndex, c_v[t], copy.deepcopy(p_v))\n",
    "\n",
    "        Seq_u = sampleDegree(uNodeIndex, c_e[t], P_su)\n",
    "        Seq_v = sampleDegree(vNodeIndex - len(uNodeIndex), c_e[t], P_sv)\n",
    "\n",
    "        bGraph = bipartite.configuration_model(Seq_u, Seq_v)\n",
    "\n",
    "        biEdgeList = nx.to_pandas_edgelist(bGraph)\n",
    "        biEdgeList['t'] = t\n",
    "        biEdgeList['isAnomaly'] = 0\n",
    "        \n",
    "    DyBAM[t] = biEdgeList  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################-----------Main code----------------######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cauchy parameters\n",
    "l_e_lower = 12\n",
    "l_e_upper = 13\n",
    "s_e_lower = 15\n",
    "s_e_upper = 16\n",
    "\n",
    "l_u_lower = 12\n",
    "l_u_upper = 13\n",
    "s_u_lower = 15\n",
    "s_u_upper = 16\n",
    "\n",
    "l_v_lower = 12\n",
    "l_v_upper = 13\n",
    "s_v_lower = 15\n",
    "s_v_upper = 16\n",
    "\n",
    "#Gamma parameters\n",
    "\n",
    "a_gu = 0.01\n",
    "l_gu = 0.75\n",
    "s_gu = 10\n",
    "\n",
    "\n",
    "a_gv = 0.001\n",
    "l_gv = 3\n",
    "s_gv = 300\n",
    "\n",
    "#Other parameters\n",
    "#Total snapshots\n",
    "T = 144\n",
    "\n",
    "#cycle duration\n",
    "x = 24\n",
    "\n",
    "#Total U nodes\n",
    "p = 200000\n",
    "\n",
    "#Total V nodes\n",
    "q = 20000\n",
    "\n",
    "#Total number of edges\n",
    "m = 1000000\n",
    "\n",
    "#Total number of cores\n",
    "c = 28\n",
    "\n",
    "#Number of cycles\n",
    "N_c = T//x\n",
    "\n",
    "#Storing graph\n",
    "#snapshot = csr_matrix((p, q), dtype=np.int32)\n",
    "\n",
    "manager = multiprocessing.Manager()\n",
    "DyBAM = manager.list(range(T))\n",
    "\n",
    "#DyBAM = np.repeat(snapshot, T)\n",
    "\n",
    "D_e = []\n",
    "D_u = []\n",
    "D_v = []\n",
    "\n",
    "y = np.linspace(0, x - 1, x, dtype = np.int32)\n",
    "\n",
    "#Generate index for U and V\n",
    "uNodeIndex = np.linspace(0, p - 1, p, dtype = np.int32)\n",
    "vNodeIndex = np.linspace(p, p + q - 1, q, dtype = np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Appending Cauchy distribution\n",
    "for i in range(N_c):\n",
    "    l_e = random.uniform(l_e_lower, l_e_upper)\n",
    "    s_e = random.uniform(s_e_lower, s_e_upper)\n",
    "    l_u = random.uniform(l_u_lower, l_u_upper)\n",
    "    s_u = random.uniform(s_u_lower, s_u_upper)\n",
    "    l_v = random.uniform(l_v_lower, l_v_upper)\n",
    "    s_v = random.uniform(s_v_lower, s_v_upper)\n",
    "    \n",
    "    D_e.extend(cauchyDistribution(y, l_e, s_e))\n",
    "    D_u.extend(cauchyDistribution(y, l_u, s_u))\n",
    "    D_v.extend(cauchyDistribution(y, l_v, s_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert back to numpy\n",
    "D_e = np.array(D_e)\n",
    "D_u = np.array(D_u)\n",
    "D_v = np.array(D_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Temporal count of edges, Node U and Node V\n",
    "C_e = probToOne(D_e) * m\n",
    "C_u = probToOne(D_u) * p\n",
    "C_v = probToOne(D_v) * q\n",
    "\n",
    "#Convert to int\n",
    "C_e = np.ceil(C_e).astype(int)\n",
    "C_u = np.ceil(C_u).astype(int)\n",
    "C_v = np.ceil(C_v).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the probability of all nodes using gamma distribution\n",
    "P_u = gamma.rvs(a = a_gu, loc = l_gu, scale = s_gu, size = p)\n",
    "P_v = gamma.rvs(a = a_gv, loc = l_gv, scale = s_gv, size = q)\n",
    "\n",
    "#Normalize the probability so that its equal to 1\n",
    "P_u = probToOne(P_u)\n",
    "P_v = probToOne(P_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of processes: 28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Each loop is processed by a different core\n",
    "jobs = []\n",
    "for i in range(T):\n",
    "\n",
    "    jobs.append([P_u, P_v, C_u, C_v, C_e, uNodeIndex, vNodeIndex, i])\n",
    "\n",
    "print(\"Number of processes: \"+str(c))\n",
    "\n",
    "stime = time.time()\n",
    "print()\n",
    "pool = Pool(processes=c)\n",
    "pool.starmap(genSS, jobs)\n",
    "pool.close()\n",
    "etime = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bicm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 60.86045813560486\n"
     ]
    }
   ],
   "source": [
    "print(\"Time taken:\", etime - stime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Feature list with number of edges(m) rows and 4 columns (u, v, t, label). \n",
    "F_org = DyBAM[0]\n",
    "\n",
    "for i in range(1, T):\n",
    "    F_org = pd.concat([F_org, DyBAM[i]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Before anomaly injection graph stats\n",
    "nU = F_org['source'].nunique()\n",
    "nV = F_org['target'].nunique()\n",
    "nE = F_org.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes U: 55792\n",
      "Number of Nodes V: 6141\n",
      "Number of Edges: 1000365\n"
     ]
    }
   ],
   "source": [
    "#Graph Stats:\n",
    "print(\"Number of Nodes U: \"+str(nU))\n",
    "print(\"Number of Nodes V: \"+str(nV))\n",
    "print(\"Number of Edges: \"+str(nE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert to numpy\n",
    "F_org = F_org.to_numpy()\n",
    "\n",
    "#Clear all zero entry\n",
    "F_org = F_org[~np.all(F_org == 0, axis=1)]\n",
    "\n",
    "F = copy.deepcopy(F_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################--------Anomaly Injection--------------################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################---------Important functions-------------######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples node and its probability for the graph snapshot\n",
    "def sampleNodesAnomaly(x, size, p):  \n",
    "    \n",
    "    p[len(p) - 1] = 1 - np.sum(p[0:len(p)-1])    \n",
    "    sampledNodes = np.random.choice(x, size, p = p, replace = False)\n",
    "    \n",
    "    return sampledNodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampleEdgeAnomaly(L_u, L_v, P_u, P_v, count):\n",
    "    \n",
    "    P_u[len(P_u) - 1] = 1 - np.sum(P_u[0:len(P_u)-1]) \n",
    "    P_v[len(P_v) - 1] = 1 - np.sum(P_v[0:len(P_v)-1]) \n",
    "    \n",
    "    sE_u = np.random.choice(L_u, count, p = P_u)\n",
    "    sE_v = np.random.choice(L_v, count, p = P_v)\n",
    "    \n",
    "    return sE_u, sE_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sum the number of edges in a graph snapshot\n",
    "def graphSum(DyBAM, t):\n",
    "    return DyBAM[t].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Update Feature Matrix\n",
    "def updateFeatureMatrix(F, L_u, L_v, t):\n",
    "    \n",
    "    #Source and destination\n",
    "    temp = np.concatenate((np.array(L_u).reshape(-1,1) , np.array(L_v).reshape(-1,1)), axis = 1).reshape(-1,2) \n",
    "    \n",
    "    #Set time \n",
    "    time_column = np.repeat(t, temp.shape[0])\n",
    "    #Set label\n",
    "    label_column = np.ones(temp.shape[0], dtype=np.int32)\n",
    "    \n",
    "    #Merge the edge list, time and label together\n",
    "    temp = np.insert(temp, 2, time_column, axis=1)\n",
    "    temp = np.insert(temp, 3, label_column, axis=1)\n",
    "    \n",
    "    #Combine it with the original edgelist\n",
    "    F = np.concatenate((F, temp), axis=0)\n",
    "    \n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################-----------Main code----------------######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flag to check if U and V are anomalous\n",
    "isU_a = True\n",
    "isV_a = True\n",
    "\n",
    "#Initial number of anomalous U and V nodes\n",
    "c_ua = 100\n",
    "c_va = 5\n",
    "\n",
    "#Anomaly subgraph properties: \n",
    "#Anomaly percentage (ratio)\n",
    "p_a = 0.1\n",
    "\n",
    "#Burstiness value\n",
    "G_b = 2\n",
    "#Propagation flag\n",
    "isG_p = True\n",
    "#Propagation ratio\n",
    "G_p = 0.01\n",
    "#Start time\n",
    "T_s = 1\n",
    "#Duration\n",
    "t_a = 47"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To minimize rounding error\n",
    "def shiftFloatingAnomalies(arr, G_b):\n",
    "    sum1 = 0\n",
    "    for i in range(len(arr)):\n",
    "        if sum1 + arr[i] < G_b:\n",
    "            sum1 += arr[i]\n",
    "            arr[i] = 0\n",
    "        else:\n",
    "            sum2 = sum1 + arr[i]\n",
    "            arr[i] = math.floor(sum2)\n",
    "            sum1 = sum2 - arr[i]\n",
    "        \n",
    "    arr[len(arr) - 1] = math.ceil(arr[len(arr) - 1] + sum1)\n",
    "    \n",
    "    return arr    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10004\n"
     ]
    }
   ],
   "source": [
    "#Number of anomalies per snapshot is also proportional to number of edges\n",
    "C_a = probToOne(D_e[T_s : (T_s + t_a)]) * (p_a * nE)\n",
    "\n",
    "C_a = shiftFloatingAnomalies(C_a, G_b).astype(int)\n",
    "\n",
    "print(sum(C_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly select nodes that are anomalous during the anomaly injection\n",
    "ML_ua = sampleNodesAnomaly(uNodeIndex, c_ua, P_u)\n",
    "ML_va = sampleNodesAnomaly(vNodeIndex, c_va, P_v)\n",
    "\n",
    "P_ua = np.repeat(1/c_ua, c_ua)\n",
    "P_va = np.repeat(1/c_va, c_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Anomaly injection function\n",
    "def anomalyInjection(\n",
    "    F,           # Edge List\n",
    "    isU_a,       # Flag if U can be anomalous\n",
    "    isV_a,       # Flag if V can be anomalous\n",
    "    c_ua,        # Number of initial U anomalous nodes\n",
    "    c_va,        # Number of initial V anomalous nodes\n",
    "    C_a,         # List of number of anomalous edges per snapshot\n",
    "    G_b,         # Burstiness ratio\n",
    "    isG_p,       # Flag if anomaly propagation is allowed\n",
    "    G_p,         # Propagation ratio\n",
    "    T_s,         # Start Time\n",
    "    t_a,         # Duration\n",
    "    uNodeIndex,  # U Node list\n",
    "    vNodeIndex   # V Node List\n",
    "):\n",
    "    #Probability of U and V node of being selected for attacking node or attacked node\n",
    "    P_u = np.repeat(1/len(uNodeIndex), len(uNodeIndex))\n",
    "    P_v = np.repeat(1/len(vNodeIndex), len(vNodeIndex))\n",
    "                        \n",
    "    # Randomly select nodes that are anomalous during the anomaly injection\n",
    "    ML_ua = sampleNodesAnomaly(uNodeIndex, c_ua, P_u)\n",
    "    ML_va = sampleNodesAnomaly(vNodeIndex, c_va, P_v)\n",
    "    \n",
    "    for i in range(t_a):\n",
    "        #Probability of Attacking nodes\n",
    "        P_ua = np.repeat(1/ML_ua.shape[0], ML_ua.shape[0])\n",
    "        P_va = np.repeat(1/ML_va.shape[0], ML_va.shape[0])\n",
    "        \n",
    "        #If both U and V nodes are attacking\n",
    "        if isU_a and isV_a and C_a[i] != 0:\n",
    "            #Calculate the number of attacks from u -> v and v -> u\n",
    "            cAu_v = (C_a[i]//2)\n",
    "            cAv_u = (C_a[i] - C_a[i]//2)\n",
    "            \n",
    "            #List of nodes that got attacked\n",
    "            L_v = sampleNodesAnomaly(vNodeIndex, cAu_v * G_b, P_v)\n",
    "            L_u = sampleNodesAnomaly(uNodeIndex, cAv_u * G_b, P_u)\n",
    "            \n",
    "            #Probability of attacked nodes\n",
    "            P_sv = np.repeat(1/(cAu_v * G_b), cAu_v * G_b)\n",
    "            P_su = np.repeat(1/(cAv_u * G_b), cAv_u * G_b)          \n",
    "            \n",
    "            #Pick U and V nodes cAu_v times from attacking U and attacked V \n",
    "            seqU, seqV = sampleEdgeAnomaly(ML_ua, L_v, P_ua, P_sv, cAu_v)\n",
    "            \n",
    "            #Update Feature List\n",
    "            F = updateFeatureMatrix(F, seqU, seqV, T_s + i)\n",
    "            \n",
    "            #Pick U and V nodes cAv_u times from attacked V and attacking U \n",
    "            seqU, seqV = sampleEdgeAnomaly(L_u, ML_va, P_su, P_va, cAv_u)\n",
    "            \n",
    "            #Update Feature List\n",
    "            F = updateFeatureMatrix(F, seqU, seqV, T_s + i)\n",
    "        \n",
    "        #If only U is attacking\n",
    "        elif isU_a and C_a[i] != 0:\n",
    "            #Calculate the number of attacks from u -> v\n",
    "            cAu_v = C_a[i]     \n",
    "            \n",
    "            #List of nodes that got attacked\n",
    "            L_v = sampleNodesAnomaly(vNodeIndex, cAu_v * G_b, P_v)\n",
    "            \n",
    "            #Probability of attacked nodes\n",
    "            P_sv = np.repeat(1/(cAu_v * G_b), cAu_v * G_b)     \n",
    "            \n",
    "            #Pick U and V nodes cAu_v times from attacking U and attacked V \n",
    "            seqU, seqV = sampleEdgeAnomaly(ML_ua, L_v, P_ua, P_sv, cAu_v)\n",
    "            \n",
    "            #Update Feature List\n",
    "            F = updateFeatureMatrix(F, seqU, seqV, T_s + i)\n",
    "        \n",
    "        #If only V is attacking\n",
    "        elif isV_a and C_a[i] != 0:\n",
    "            #Calculate the number of attacks from v -> u\n",
    "            cAv_u = C_a[i]        \n",
    "            \n",
    "            #List of nodes that got attacked\n",
    "            L_u = sampleNodesAnomaly(uNodeIndex, cAv_u * G_b, P_u)\n",
    "            \n",
    "            #Probability of attacked nodes\n",
    "            P_su = np.repeat(1/(cAv_u * G_b), cAv_u * G_b)      \n",
    "            \n",
    "            #Pick U and V nodes cAv_u times from attacked V and attacking U\n",
    "            seqU, seqV = sampleEdgeAnomaly(L_u, ML_va, P_su, P_va, cAv_u)\n",
    "            \n",
    "            #Update Feature List\n",
    "            F = updateFeatureMatrix(F, seqU, seqV, T_s + i)            \n",
    "\n",
    "        #Only a portion of attacked nodes (propagation ratio) will be infected. Add the infected nodes to the list of existing infected nodes\n",
    "        if isG_p:\n",
    "            if isU_a:\n",
    "                np.append(ML_ua, sampleNodesAnomaly(L_u, int(G_p * len(L_u)), np.repeat(1/len(L_u), len(L_u))))\n",
    "\n",
    "            if isV_a:\n",
    "                np.append(ML_va, sampleNodesAnomaly(L_v, int(G_p * len(L_v)), np.repeat(1/len(L_v), len(L_v))))\n",
    "                \n",
    "    return F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [148], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Inject anomaly\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m F \u001b[38;5;241m=\u001b[39m \u001b[43manomalyInjection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misU_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misV_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_ua\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_va\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43misG_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mT_s\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muNodeIndex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvNodeIndex\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [147], line 62\u001b[0m, in \u001b[0;36manomalyInjection\u001b[0;34m(F, isU_a, isV_a, c_ua, c_va, C_a, G_b, isG_p, G_p, T_s, t_a, uNodeIndex, vNodeIndex)\u001b[0m\n\u001b[1;32m     59\u001b[0m cAu_v \u001b[38;5;241m=\u001b[39m C_a[i]     \n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m#List of nodes that got attacked\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m L_v \u001b[38;5;241m=\u001b[39m \u001b[43msampleNodesAnomaly\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvNodeIndex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcAu_v\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mG_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mP_v\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m#Probability of attacked nodes\u001b[39;00m\n\u001b[1;32m     65\u001b[0m P_sv \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(cAu_v \u001b[38;5;241m*\u001b[39m G_b), cAu_v \u001b[38;5;241m*\u001b[39m G_b)     \n",
      "Cell \u001b[0;32mIn [138], line 5\u001b[0m, in \u001b[0;36msampleNodesAnomaly\u001b[0;34m(x, size, p)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msampleNodesAnomaly\u001b[39m(x, size, p):  \n\u001b[1;32m      4\u001b[0m     p[\u001b[38;5;28mlen\u001b[39m(p) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39msum(p[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;28mlen\u001b[39m(p)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])    \n\u001b[0;32m----> 5\u001b[0m     sampledNodes \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sampledNodes\n",
      "File \u001b[0;32mmtrand.pyx:975\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "#Inject anomaly\n",
    "F = anomalyInjection(F, isU_a, isV_a, c_ua, c_va, C_a, G_b, isG_p, G_p, T_s, t_a, uNodeIndex, vNodeIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final graph stats\n",
    "nU = np.unique(F[:, 0]).shape[0]\n",
    "nV = np.unique(F[:, 1]).shape[0]\n",
    "nE = F.shape[0]\n",
    "nA = F[F[:, 3] == 1].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Nodes U: 76223\n",
      "Number of Nodes V: 18633\n",
      "Number of Edges: 1100079\n",
      "Number of Anomalies: 100008\n"
     ]
    }
   ],
   "source": [
    "#Graph Stats:\n",
    "print(\"Number of Nodes U: \"+str(nU))\n",
    "print(\"Number of Nodes V: \"+str(nV))\n",
    "print(\"Number of Edges: \"+str(nE))\n",
    "print(\"Number of Anomalies: \"+str(nA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################-------Attributes Generation-----------#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datasetName = 'Reddit'\n",
    "datasetName = 'pcore'\n",
    "#datasetName = 'Wiki'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################----------Sampling----------------#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrainData = pd.read_csv('../Dataset/'+ datasetName + 'Normal.csv')\n",
    "aTrainData = pd.read_csv('../Dataset/'+ datasetName + 'Mal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skip this if you have pre-trained model\n",
    "#Training normal dataset\n",
    "model_normal = CTGAN(\n",
    "    verbose = True,\n",
    "    cuda = True\n",
    ")\n",
    "model_normal.fit(nTrainData)\n",
    "model_normal.save(datasetName + 'Normal.pkl')\n",
    "\n",
    "#Training anomaly dataset\n",
    "model_anomaly = CTGAN(\n",
    "    verbose = True,\n",
    "    cuda = True\n",
    ")\n",
    "model_anomaly.fit(aTrainData)\n",
    "model_anomaly.save(datasetName +'Mal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load from the trained model\n",
    "model_normal = CTGAN.load('../Saved_models/'+ datasetName + 'Normal.pkl')\n",
    "\n",
    "model_anomaly = CTGAN.load('../Saved_models/'+ datasetName + 'Mal.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate normal Attributes\n",
    "attributes_normal = model_normal.sample(num_rows=(nE - nA))\n",
    "\n",
    "#Generate anomaly attributes\n",
    "attributes_anomaly = model_anomaly.sample(num_rows=nA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################----------Mapping----------------#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normal = pd.DataFrame(F[F[:,3] == 0], columns = ['src', 'dest', 't', 'isAnomaly'])\n",
    "df_anomaly = pd.DataFrame(F[F[:,3] == 1], columns = ['src', 'dest', 't', 'isAnomaly'])\n",
    "\n",
    "df_normal = pd.concat([df_normal, attributes_normal], axis=1)\n",
    "df_anomaly = pd.concat([df_anomaly, attributes_anomaly], axis=1)\n",
    "\n",
    "df_final = pd.concat([df_normal, df_anomaly], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "id=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(datasetName + '_Syn'+'genID_'+str(id)+'.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (prml)",
   "language": "python",
   "name": "prml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
